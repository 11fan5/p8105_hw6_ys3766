---
title: "p8105_hw6_ys3766"
author: "Yifan Shi"
date: "2024-12-03"
output: github_document
---

```{r, include = FALSE, message = FALSE, warning = FALSE}
library(tidyverse)
library(broom)
library(modelr)

knitr::opts_chunk$set(
	echo = TRUE,
	warning = FALSE,
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

# Question 1
```{r}
weather_df = 
  rnoaa::meteo_pull_monitors(
    c("USW00094728"),
    var = c("PRCP", "TMIN", "TMAX"), 
    date_min = "2017-01-01",
    date_max = "2017-12-31") %>%
  mutate(
    name = recode(id, USW00094728 = "CentralPark_NY"),
    tmin = tmin / 10,
    tmax = tmax / 10) %>%
  select(name, id, everything())
```

Generate 5000 bootstrap samples
```{r}
set.seed(123)
bootstrap_samples <- modelr::bootstrap(weather_df, 5000)
```

Fit models and compute metrics for each bootstrap sample
```{r}
bootstrap_analysis <- bootstrap_samples %>%
  mutate(
    linear_model = map(strap, ~ lm(tmax ~ tmin, data = as_tibble(.x))),
    r_squared_values = map_dbl(linear_model, function(model) glance(model)[["r.squared"]]),
    log_coefficients_product = map_dbl(linear_model, function(model) {
      coefficients <- tidy(model) %>% pull(estimate)
      log(coefficients[1] * coefficients[2])
    })
  ) %>%
  select(r_squared_values, log_coefficients_product)
```

Plot distributions of r-squared and log(beta0 * beta1)
```{r}
bootstrap_analysis %>%
  pivot_longer(cols = everything(), names_to = "metric", values_to = "value") %>%
  ggplot(aes(x = value, fill = metric)) +
  geom_density(alpha = 0.5) +
  facet_wrap(vars(metric), scales = "free", ncol = 1) +
  labs(
    title = "Bootstrap Distributions of R² and log(β0 × β1)",
    x = NULL,
    y = "Density"
  )
```

The upper plot of log(beta0 x beta1) is symmetrically distributed, centered slightly above 2.00, with values ranging from 1.95 to 2.05. This shows moderate variability in the regression coefficients across samples, but supports the stability of the model parameters and a good over all fit. 

The lower plot of R^2 shows a symmetric distribution concentrating between 0.88 and 0.93, with a peak at around 0.91. This indicates a consistently strong model fit across the bootstrap samples, with low variability, suggesting the model reliably explains the variance in tmax using tmin. 

95% CI
```{r}
bootstrap_analysis %>%
  summarise(
    r_squared_lower = quantile(r_squared_values, 0.025),
    r_squared_upper = quantile(r_squared_values, 0.975),
    log_coefficients_product_lower = quantile(log_coefficients_product, 0.025),
    log_coefficients_product_upper = quantile(log_coefficients_product, 0.975)
  ) %>% knitr::kable(digits = 3)
```

# Question 2
import data, clean and prepare for analysi 
```{r}
homicide_df <- read_csv(file = "data/homicide-data.csv", na = c("Unknown", "NA", "")) %>%
  mutate(
    city_state = str_c(city, state, sep = ", "),
    solved = if_else(disposition == "Closed by arrest", 1, 0),
    victim_age = as.numeric(victim_age),
    reported_date = as.Date(as.character(reported_date), format = "%Y%m%d"),
  ) %>%
  filter(
    !city_state %in% c("Dallas, TX", "Phoenix, AZ", "Kansas City, MO", "Tulsa, AL"),
    victim_race %in% c("White", "Black")
  )
```

Logistic Regression for Baltimore, MD
```{r}

baltimore_results <- homicide_df %>%
  filter(city_state == "Baltimore, MD") %>%
  glm(solved ~ victim_age + victim_sex + victim_race, family = binomial(), data = .) %>%
  broom::tidy(conf.int = TRUE) %>%
  filter(term == "victim_sexMale") %>%
  mutate(
    OR = exp(estimate),       
    CI_lower = exp(conf.low),   
    CI_upper = exp(conf.high)  
  ) %>%
  select(OR, CI_lower, CI_upper)

knitr::kable(baltimore_results, digits = 3, caption = "Adjusted OR for Male vs Female Victims in Baltimore, MD")

```

Logistic Regression for Each City
```{r}
city_results <- homicide_df %>%
  group_by(city_state) %>%
  nest() %>%
  mutate(
    model = map(data, ~ glm(solved ~ victim_age + victim_sex + victim_race, family = binomial(), data = .x)),
    results = map(model, ~ broom::tidy(.x, conf.int = TRUE) %>%
                    filter(term == "victim_sexMale") %>%
                    mutate(
                      OR = exp(estimate),
                      ci_lower = exp(conf.low),
                      ci_upper = exp(conf.high)
                    ) %>%
                    select(OR, ci_lower, ci_upper))
  ) %>%
  unnest(results) %>%
  select(city_state, OR, ci_lower, ci_upper)

knitr::kable(city_results, digits = 3, caption = "Adjusted ORs for Male vs Female Victims Across Cities")


```

Plot Adjusted Odds Ratios and Confidence Intervals

```{r fig.width=10, fig.height=8}
city_results %>%
  ggplot(aes(x = reorder(city_state, OR), y = OR)) +
  geom_point() +
  geom_errorbar(aes(ymin = ci_lower, ymax = ci_upper), width = 0.2) +
  coord_flip() +
  labs(
    title = "Estimated ORs for Male vs Female Victims Across Cities",
    x = "City, State",
    y = "Odds Ratio (Male vs Female Victims)"
  )
```

The plot shows adjusted odds ratios (ORs) for solving homicides involving male vs. female victims across cities, with 95% confidence intervals (CIs). ORs > 1 suggest male homicides are more likely solved, while ORs < 1 indicate the opposite. Many cities cluster around an OR of 1, showing no significant gender-based differences, as their CIs overlap 1. Cities like New York, NY, and Chicago, IL, have narrow CIs, reflecting precise estimates due to larger sample sizes, whereas cities like Albuquerque, NM, Stockton, CA, and Fresno, CA, have wide CIs, reflecting high uncertainty due to smaller samples or variability. While most cities show no significant gender bias, the variability in cities with extreme ORs or wide CIs, such as Albuquerque, NM, and Stockton, CA, warrants further investigation.

# Question 3
Load and clean the dataset
```{r}
birthweight_df <- read_csv("data/birthweight.csv") %>%
  janitor::clean_names() %>%  
  mutate(
    babysex = factor(babysex, levels = c(1, 2), labels = c("male", "female")),
    frace = factor(frace, levels = c(1, 2, 3, 4, 8, 9), 
                   labels = c("White", "Black", "Asian", "Puerto Rican", "Other", "Unknown")),
    mrace = factor(mrace, levels = c(1, 2, 3, 4, 8), 
                   labels = c("White", "Black", "Asian", "Puerto Rican", "Other")),
    malform = as.logical(malform)  
  ) %>%
  drop_na()

```

Regression model

```{r}
full_model <- lm(
  bwt ~ babysex + bhead + blength + delwt + fincome + frace + gaweeks +
    malform + menarche + mheight + momage + mrace + parity +
    pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain,
  data = birthweight_df
)

# Backward elimination
stepwise_model <- step(full_model, direction = "backward", trace = 0)

summary(stepwise_model)
```

Residual Analysis
```{r}
birthweight_df <- birthweight_df %>%
  mutate(
    predicted_bwt = predict(stepwise_model, newdata = .),
    residuals_bwt = residuals(stepwise_model)
  )

ggplot(birthweight_df, aes(x = predicted_bwt, y = residuals_bwt)) +
  geom_point(alpha = 0.5) +
  geom_smooth(method = "loess") +
  labs(
    title = "Residuals vs Fitted Values",
    x = "Fitted Values (Predicted Birthweight)",
    y = "Residuals"
  ) +
  theme_minimal()
```

To predict birthweight, the proposed model incorporates a combination of baby characteristics, maternal factors, and pregnancy-specific variables. This comprehensive approach reflects the biological and socioeconomic factors influencing fetal growth. Key predictors include baby sex, head circumference, and length at birth, which directly capture physical growth. Maternal characteristics such as weight at delivery, height, and smoking behavior are included as they significantly affect maternal and fetal health. Socioeconomic variables like family income and race provide proxies for healthcare access and resources. Finally, pregnancy-specific predictors like gestational age, parity, and weight gain during pregnancy address the broader context of fetal development. After applying backward elimination to refine the model, the adjusted model achieved an R^2 of approximately 0.72, indicating a strong fit to the data. Residual diagnostics showed no major deviations from linear regression assumptions.

The residuals vs. fitted values plot shows that residuals are centered around zero, indicating no systematic bias in the predictions. However, a slight curvature at lower fitted values suggests potential non-linearity that could be better addressed with alternative modeling techniques, such as non-linear regression or interaction terms. Despite this, the overall distribution of residuals supports the validity of the linear model for predicting birthweight.



cross validation
```{r}
model_1 <- lm(bwt ~ blength + gaweeks, data = birthweight_df)
model_2 <- lm(bwt ~ bhead * blength * babysex, data = birthweight_df)

set.seed(123)
cv_results <- crossv_mc(birthweight_df, 100) %>%
  mutate(
    train = map(train, as_tibble),
    test = map(test, as_tibble),
    model_1 = map(train, ~ lm(bwt ~ blength + gaweeks, data = .x)),
    model_2 = map(train, ~ lm(bwt ~ bhead * blength * babysex, data = .x)),
    stepwise_model = map(train, ~ lm(
      bwt ~ babysex + bhead + blength + delwt + fincome + frace +
        gaweeks + malform + menarche + mheight + momage + mrace +
        parity + pnumlbw + pnumsga + ppbmi + ppwt + smoken + wtgain,
      data = .x
    ))
  ) %>%
  mutate(
    rmse_1 = map2_dbl(model_1, test, ~ rmse(.x, .y)),
    rmse_2 = map2_dbl(model_2, test, ~ rmse(.x, .y)),
    rmse_stepwise = map2_dbl(stepwise_model, test, ~ rmse(.x, .y))
  )

cv_summary <- cv_results %>%
  select(rmse_1, rmse_2, rmse_stepwise) %>%
  pivot_longer(cols = everything(), names_to = "model", values_to = "rmse") %>%
  mutate(model = recode(model,
                        "rmse_1" = "Model 1: Length + Gestational Age",
                        "rmse_2" = "Model 2: Head Circumference + Length + Sex",
                        "rmse_stepwise" = "Stepwise Model")) %>%
  group_by(model) %>%
  summarize(mean_rmse = mean(rmse), sd_rmse = sd(rmse))

cv_summary %>% knitr::kable(digits = 3, caption = "Cross-Validation RMSE Summary")
```
Three models were compared to assess predictive performance using Monte Carlo cross-validation with 100 iterations:

Model 1 (Length at Birth + Gestational Age):
This simplest model showed the poorest predictive performance, as indicated by its high root mean squared error (RMSE) and wide variability. It lacks sufficient predictors to capture the complex relationships influencing birthweight.

Model 2 (Head Circumference + Length + Sex with Interactions):
This intermediate model significantly outperformed Model 1. By including head circumference, sex, and their interactions, it achieved better accuracy with lower RMSE and less variability. It balances simplicity and predictive power.

Stepwise Model (Full Model):
The stepwise regression model exhibited the lowest RMSE and most consistent predictions across iterations, reflecting its ability to incorporate the multifactorial influences on birthweight. However, the added complexity of this model may limit interpretability in practical applications.

Violin plot
```{r fig.width=12, fig.height=10}
cv_results %>%
  select(rmse_1, rmse_2, rmse_stepwise) %>%
  pivot_longer(cols = everything(), names_to = "model", values_to = "rmse") %>%
  mutate(model = recode(model,
                        "rmse_1" = "Model 1",
                        "rmse_2" = "Model 2",
                        "rmse_stepwise" = "Stepwise Model")) %>%
  ggplot(aes(x = model, y = rmse, fill = model)) +
  geom_violin(alpha = 0.7) +
  labs(
    title = "Cross-Validation RMSE Distributions for Models",
    x = "Model",
    y = "RMSE"
  ) 
```

The violin plot of RMSE distributions clearly shows that the stepwise model achieves the best predictive performance, while Model 2 offers a more interpretable alternative with slightly lower predictive power. Model 1 is insufficient for accurate predictions, highlighting the importance of including a broader range of predictors.

The stepwise regression model is the most accurate for predicting birthweight, making it ideal for applications requiring precision, such as research studies or clinical decision-making. However, its complexity may limit its use in settings where interpretability is critical. Model 2 provides a strong alternative, offering a balance between simplicity and accuracy, making it suitable for public health or policy applications. Ultimately, the choice of model depends on the specific context and the need to balance interpretability, complexity, and predictive performance.
